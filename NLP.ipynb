{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT2AMJKRpmkzlXa9h2cndC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CODERdeeps/MLCDAC/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXOLUrIixQRn",
        "outputId": "87125de9-da2f-45b6-80f0-67178477911a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "##!\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "# Text\n",
        "text = \"Viruses are constantly evolving and changing. Every time a virus replicates, there is the potential for there to be changes in its structure. Each of these changes is a mutation. A virus with one or more mutations is called a variant of the original virus. Some mutations can lead to changes in important characteristics of the virus, including characteristics that affect its ability to spread and its ability to cause more severe illness and death.\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)\n",
        "\n",
        "# Remove Stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "print(\"\\nAfter removing stopwords:\")\n",
        "print(filtered_words)\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"\\nPOS Tagging:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"\\nAfter stemming:\")\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mVLiSHmxy0G",
        "outputId": "aa74b7e2-a85a-464f-ba59-9e9a52948a5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "Viruses are constantly evolving and changing.\n",
            "Every time a virus replicates, there is the potential for there to be changes in its structure.\n",
            "Each of these changes is a mutation.\n",
            "A virus with one or more mutations is called a variant of the original virus.\n",
            "Some mutations can lead to changes in important characteristics of the virus, including characteristics that affect its ability to spread and its ability to cause more severe illness and death.\n",
            "\n",
            "Word Tokenization:\n",
            "['Viruses', 'are', 'constantly', 'evolving', 'and', 'changing', '.', 'Every', 'time', 'a', 'virus', 'replicates', ',', 'there', 'is', 'the', 'potential', 'for', 'there', 'to', 'be', 'changes', 'in', 'its', 'structure', '.', 'Each', 'of', 'these', 'changes', 'is', 'a', 'mutation', '.', 'A', 'virus', 'with', 'one', 'or', 'more', 'mutations', 'is', 'called', 'a', 'variant', 'of', 'the', 'original', 'virus', '.', 'Some', 'mutations', 'can', 'lead', 'to', 'changes', 'in', 'important', 'characteristics', 'of', 'the', 'virus', ',', 'including', 'characteristics', 'that', 'affect', 'its', 'ability', 'to', 'spread', 'and', 'its', 'ability', 'to', 'cause', 'more', 'severe', 'illness', 'and', 'death', '.']\n",
            "\n",
            "After removing stopwords:\n",
            "['Viruses', 'constantly', 'evolving', 'changing', '.', 'Every', 'time', 'virus', 'replicates', ',', 'potential', 'changes', 'structure', '.', 'changes', 'mutation', '.', 'virus', 'one', 'mutations', 'called', 'variant', 'original', 'virus', '.', 'mutations', 'lead', 'changes', 'important', 'characteristics', 'virus', ',', 'including', 'characteristics', 'affect', 'ability', 'spread', 'ability', 'cause', 'severe', 'illness', 'death', '.']\n",
            "\n",
            "POS Tagging:\n",
            "[('Viruses', 'NNS'), ('are', 'VBP'), ('constantly', 'RB'), ('evolving', 'VBG'), ('and', 'CC'), ('changing', 'VBG'), ('.', '.'), ('Every', 'JJ'), ('time', 'NN'), ('a', 'DT'), ('virus', 'NN'), ('replicates', 'VBZ'), (',', ','), ('there', 'EX'), ('is', 'VBZ'), ('the', 'DT'), ('potential', 'JJ'), ('for', 'IN'), ('there', 'EX'), ('to', 'TO'), ('be', 'VB'), ('changes', 'NNS'), ('in', 'IN'), ('its', 'PRP$'), ('structure', 'NN'), ('.', '.'), ('Each', 'DT'), ('of', 'IN'), ('these', 'DT'), ('changes', 'NNS'), ('is', 'VBZ'), ('a', 'DT'), ('mutation', 'NN'), ('.', '.'), ('A', 'DT'), ('virus', 'NN'), ('with', 'IN'), ('one', 'CD'), ('or', 'CC'), ('more', 'JJR'), ('mutations', 'NNS'), ('is', 'VBZ'), ('called', 'VBN'), ('a', 'DT'), ('variant', 'NN'), ('of', 'IN'), ('the', 'DT'), ('original', 'JJ'), ('virus', 'NN'), ('.', '.'), ('Some', 'DT'), ('mutations', 'NNS'), ('can', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('changes', 'NNS'), ('in', 'IN'), ('important', 'JJ'), ('characteristics', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('virus', 'NN'), (',', ','), ('including', 'VBG'), ('characteristics', 'NNS'), ('that', 'WDT'), ('affect', 'VBP'), ('its', 'PRP$'), ('ability', 'NN'), ('to', 'TO'), ('spread', 'VB'), ('and', 'CC'), ('its', 'PRP$'), ('ability', 'NN'), ('to', 'TO'), ('cause', 'VB'), ('more', 'JJR'), ('severe', 'JJ'), ('illness', 'NN'), ('and', 'CC'), ('death', 'NN'), ('.', '.')]\n",
            "\n",
            "After stemming:\n",
            "['virus', 'constantli', 'evolv', 'chang', '.', 'everi', 'time', 'viru', 'replic', ',', 'potenti', 'chang', 'structur', '.', 'chang', 'mutat', '.', 'viru', 'one', 'mutat', 'call', 'variant', 'origin', 'viru', '.', 'mutat', 'lead', 'chang', 'import', 'characterist', 'viru', ',', 'includ', 'characterist', 'affect', 'abil', 'spread', 'abil', 'caus', 'sever', 'ill', 'death', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##2"
      ],
      "metadata": {
        "id": "_LBG-bZwy3aL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Pfizer-vaccin_tweets.csv')\n"
      ],
      "metadata": {
        "id": "UtPRaeF3y6uh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Extract hashtags and count\n",
        "hashtags = data['text'].apply(lambda x: re.findall(r'#\\w+', x))\n",
        "hashtags_count = sum(hashtags.str.len())\n",
        "\n",
        "# Extract @ tags and count\n",
        "tags = data['text'].apply(lambda x: re.findall(r'@\\w+', x))\n",
        "tags_count = sum(tags.str.len())\n",
        "\n",
        "print(\"Total Hashtags:\", hashtags_count)\n",
        "print(\"Total @ Tags:\", tags_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNg5tdHbzsqZ",
        "outputId": "aa6c0002-884d-4f01-ffc7-07b510943f3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Hashtags: 4888\n",
            "Total @ Tags: 1223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters and symbols\n",
        "data['clean_text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# Display cleaned text\n",
        "print(data['clean_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVgPKgWazv0T",
        "outputId": "e4d9e65c-5a12-404d-d67d-bf97c4f2bd72"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       Same folks said daikon paste could treat a cyt...\n",
            "1       While the world has been on the wrong side of ...\n",
            "2       coronavirus SputnikV AstraZeneca PfizerBioNTec...\n",
            "3       Facts are immutable Senator even when youre no...\n",
            "4       Explain to me again why we need a vaccine Bori...\n",
            "                              ...                        \n",
            "2505    Portuguese woman dies two days after getting P...\n",
            "2506    Sanna Elkadiri a nursing home worker from Eind...\n",
            "2507    Coronavirus vaccine update When will corona me...\n",
            "2508    I just felt a little prick  CovidVaccine Pfize...\n",
            "2509    divadarlings1 DrUnaDuffy trishgreenhalgh 1 you...\n",
            "Name: clean_text, Length: 2510, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['clean_text'] = data['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
        "\n",
        "# Display cleaned text\n",
        "print(data['clean_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prYH06SEzynO",
        "outputId": "86aaac96-abee-4019-a424-5bb881068434"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       folks said daikon paste could treat cytokine s...\n",
            "1       world wrong side history year hopefully bigges...\n",
            "2       coronavirus SputnikV AstraZeneca PfizerBioNTec...\n",
            "3       Facts immutable Senator even youre ethically s...\n",
            "4       Explain need vaccine BorisJohnson MattHancock ...\n",
            "                              ...                        \n",
            "2505    Portuguese woman dies two days getting Pfizer ...\n",
            "2506    Sanna Elkadiri nursing home worker Eindhoven b...\n",
            "2507    Coronavirus vaccine update corona medicine rea...\n",
            "2508    felt little prick CovidVaccine PfizerBioNTech ...\n",
            "2509    divadarlings1 DrUnaDuffy trishgreenhalgh 1 100...\n",
            "Name: clean_text, Length: 2510, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the WordNet lemmatizer if not already downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize words\n",
        "data['lemma_text'] = data['clean_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "# Display lemmatized text\n",
        "print(data['lemma_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy1tGRw_z2E8",
        "outputId": "5c76edce-8488-4449-8a28-f9a7736855d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       folk said daikon paste could treat cytokine st...\n",
            "1       world wrong side history year hopefully bigges...\n",
            "2       coronavirus SputnikV AstraZeneca PfizerBioNTec...\n",
            "3       Facts immutable Senator even youre ethically s...\n",
            "4       Explain need vaccine BorisJohnson MattHancock ...\n",
            "                              ...                        \n",
            "2505    Portuguese woman dy two day getting Pfizer cov...\n",
            "2506    Sanna Elkadiri nursing home worker Eindhoven b...\n",
            "2507    Coronavirus vaccine update corona medicine rea...\n",
            "2508    felt little prick CovidVaccine PfizerBioNTech ...\n",
            "2509    divadarlings1 DrUnaDuffy trishgreenhalgh 1 100...\n",
            "Name: lemma_text, Length: 2510, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##3"
      ],
      "metadata": {
        "id": "pSAHHoxDz4os"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/SMS-spam-collection-dataset.csv', encoding='latin-1')\n",
        "\n",
        "# Clean the text data\n",
        "def clean_text(v2):\n",
        "    # Remove special characters and symbols\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', v2)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    v2 = v2.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = v2.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the cleaned words back into a sentence\n",
        "    cleaned_text = ' '.join(words)\n",
        "    \n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the cleaning function to the 'text' column\n",
        "data['cleaned_text'] = data['v2'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "9pzpUUccz6yE"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}